{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "186b954f-dd38-495a-a5e7-30b0cced502b",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_dir = '/glade/work/kjmayer/research/catalyst/S2S_ocn_lnd_atm/'\n",
    "import xarray as xr\n",
    "import numpy as np\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torchinfo\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "from model.train_utils import UNet\n",
    "import utils.utils\n",
    "from utils.utils import get_config\n",
    "from utils.utils import prepare_device\n",
    "from trainer.trainer import Trainer\n",
    "import model.metrics as module_metric\n",
    "from data_prep.data_loader import GetXData, GetYData, CustomDataset,lead_shift, concat_input\n",
    "\n",
    "# torch.cuda.is_available()\n",
    "# torch.cuda.device_count()\n",
    "# torch.cuda.current_device()\n",
    "# torch.cuda.device(0)\n",
    "torch.cuda.get_device_name(0)\n",
    "\n",
    "config = get_config(\"exp_1\")\n",
    "torch.manual_seed(config[\"seed\"])\n",
    "torch.cuda.manual_seed(config[\"seed\"])\n",
    "np.random.seed(config[\"seed\"])\n",
    "random.seed(config[\"seed\"])\n",
    "torch.backends.cudnn.deterministic = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3e0c151f-25a5-4d4c-ad70-d1f3ddd432a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/glade/work/wchapman/miniconda3.1/envs/MLWPS/lib/python3.11/site-packages/xarray/core/variable.py:1546: PerformanceWarning: Reshaping is producing a large chunk. To accept the large\n",
      "chunk and silence this warning, set the option\n",
      "    >>> with dask.config.set(**{'array.slicing.split_large_chunks': False}):\n",
      "    ...     array.reshape(shape)\n",
      "\n",
      "To avoid creating the large chunks, set the option\n",
      "    >>> with dask.config.set(**{'array.slicing.split_large_chunks': True}):\n",
      "    ...     array.reshape(shape)Explictly passing ``limit`` to ``reshape`` will also silence this warning\n",
      "    >>> array.reshape(shape, limit='128 MiB')\n",
      "  result = result._stack_once(dims, new_dim)\n",
      "/glade/work/wchapman/miniconda3.1/envs/MLWPS/lib/python3.11/site-packages/xarray/core/variable.py:1546: PerformanceWarning: Reshaping is producing a large chunk. To accept the large\n",
      "chunk and silence this warning, set the option\n",
      "    >>> with dask.config.set(**{'array.slicing.split_large_chunks': False}):\n",
      "    ...     array.reshape(shape)\n",
      "\n",
      "To avoid creating the large chunks, set the option\n",
      "    >>> with dask.config.set(**{'array.slicing.split_large_chunks': True}):\n",
      "    ...     array.reshape(shape)Explictly passing ``limit`` to ``reshape`` will also silence this warning\n",
      "    >>> array.reshape(shape, limit='128 MiB')\n",
      "  result = result._stack_once(dims, new_dim)\n",
      "/glade/work/wchapman/miniconda3.1/envs/MLWPS/lib/python3.11/site-packages/xarray/core/variable.py:1546: PerformanceWarning: Reshaping is producing a large chunk. To accept the large\n",
      "chunk and silence this warning, set the option\n",
      "    >>> with dask.config.set(**{'array.slicing.split_large_chunks': False}):\n",
      "    ...     array.reshape(shape)\n",
      "\n",
      "To avoid creating the large chunks, set the option\n",
      "    >>> with dask.config.set(**{'array.slicing.split_large_chunks': True}):\n",
      "    ...     array.reshape(shape)Explictly passing ``limit`` to ``reshape`` will also silence this warning\n",
      "    >>> array.reshape(shape, limit='128 MiB')\n",
      "  result = result._stack_once(dims, new_dim)\n",
      "/glade/work/wchapman/miniconda3.1/envs/MLWPS/lib/python3.11/site-packages/xarray/core/variable.py:1546: PerformanceWarning: Reshaping is producing a large chunk. To accept the large\n",
      "chunk and silence this warning, set the option\n",
      "    >>> with dask.config.set(**{'array.slicing.split_large_chunks': False}):\n",
      "    ...     array.reshape(shape)\n",
      "\n",
      "To avoid creating the large chunks, set the option\n",
      "    >>> with dask.config.set(**{'array.slicing.split_large_chunks': True}):\n",
      "    ...     array.reshape(shape)Explictly passing ``limit`` to ``reshape`` will also silence this warning\n",
      "    >>> array.reshape(shape, limit='128 MiB')\n",
      "  result = result._stack_once(dims, new_dim)\n"
     ]
    }
   ],
   "source": [
    "LEAD = config[\"data_loader\"][\"lead\"][0] # will loop over this for training eventually\n",
    "trainfinames = config[\"data_loader\"][\"anommems_finames\"][0:6]\n",
    "valfinames = config[\"data_loader\"][\"anommems_finames\"][6:8]\n",
    "# xtrain, xtrainmean, xtrainstd, xtrainmin, xtrainmax\n",
    "xtrain, xtrainmin, xtrainmax = GetXData(dir=config[\"data_loader\"][\"base_dir\"],\n",
    "                                      var=config[\"data_loader\"][\"atm_var\"],\n",
    "                                      finames=trainfinames,\n",
    "                                      train=True,\n",
    "                                      climo=False)[0]\n",
    "# xtrain = xtrain.stack(l=('lat','lon'))\n",
    "xtrain_shift = lead_shift(xtrain, lead=LEAD, forward=False)\n",
    "\n",
    "#ytrain, ytrainmean, ytrainstd, ytrainmax, ytrainmin\n",
    "ytrain, ytrainmax, ytrainmin = GetYData(dir=config[\"data_loader\"][\"base_dir\"],\n",
    "                                       var=config[\"data_loader\"][\"atm_var\"],\n",
    "                                       finames=trainfinames,\n",
    "                                       train=True,\n",
    "                                       norm=True,\n",
    "                                       )[0]\n",
    "\n",
    "# ytrain = ytrain.sel(lat=slice(29,61))\n",
    "# ytrain = ytrain.stack(l=('lat','lon'))\n",
    "ytrain_shift = lead_shift(ytrain, lead=LEAD, forward=True)\n",
    "\n",
    "xval = GetXData(dir=config[\"data_loader\"][\"base_dir\"],\n",
    "                var=config[\"data_loader\"][\"atm_var\"],\n",
    "                finames=valfinames,\n",
    "                train=False,\n",
    "                # trainmean=xtrainmean,\n",
    "                # trainstd=xtrainstd,\n",
    "                trainmin=xtrainmin,\n",
    "                trainmax=xtrainmax,\n",
    "                climo=False)[0]\n",
    "# \n",
    "xval_shift = lead_shift(xval, lead=LEAD, forward=False)\n",
    "\n",
    "yval = GetYData(dir=config[\"data_loader\"][\"base_dir\"],\n",
    "                var=config[\"data_loader\"][\"atm_var\"],\n",
    "                finames=trainfinames,\n",
    "                train=False,\n",
    "                # trainmean=ytrainmean,\n",
    "                # trainstd=ytrainstd,\n",
    "                norm=True,\n",
    "                trainmax=ytrainmax,\n",
    "                trainmin=ytrainmin,\n",
    "                )[0]\n",
    "\n",
    "# yval = yval.sel(lat=slice(29,61))\n",
    "# yval = yval.stack(l=('lat','lon'))\n",
    "yval_shift = lead_shift(yval, lead=LEAD, forward=True)\n",
    "\n",
    "\n",
    "# climo (same for train, val, and test --> basically a DOY encoder)\n",
    "xclimo, climomin, climomax = GetXData(dir = config[\"data_loader\"][\"base_dir\"],\n",
    "                                     var = config[\"data_loader\"][\"atm_var\"],\n",
    "                                     finames = config[\"data_loader\"][\"climo_finame\"],\n",
    "                                     train = False, # MUST ALWAYS BE False FOR CLIMO\n",
    "                                     climo = True)[0]\n",
    "# climo appended to same length as training\n",
    "xclimo = xclimo.rename({'dayofyear': 'time'})\n",
    "xclimo_train = xr.concat([xclimo]*int(xtrain.shape[0]/365),dim='mem')\n",
    "xclimo_train = xclimo_train.stack(s=('mem','time')).transpose('s', 'lat', 'lon').reset_index(['s'])\n",
    "xclimo_train_shift = lead_shift(xclimo_train,lead=LEAD,forward=False)\n",
    "\n",
    "# climo appended to same length as validation\n",
    "xclimo_val = xr.concat([xclimo]*int(xval.shape[0]/365),dim='mem')\n",
    "xclimo_val = xclimo_val.stack(s=('mem','time')).transpose('s', 'lat', 'lon').reset_index(['s'])\n",
    "xclimo_val_shift = lead_shift(xclimo_val,lead=LEAD,forward=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fea1995c-49af-4621-8b7f-3b7a10d2659a",
   "metadata": {},
   "outputs": [],
   "source": [
    "Xtrain = concat_input(xtrain_shift,xclimo_train_shift,dim_name='features').values\n",
    "# Xtrain = xtrain_shift.expand_dims(dim={\"features\": 1},axis=1).values\n",
    "del xtrain_shift,xclimo_train_shift\n",
    "Xval = concat_input(xval_shift,xclimo_val_shift,dim_name='features').values\n",
    "# Xval = xval_shift.expand_dims(dim={\"features\": 1},axis=1).values\n",
    "del xval_shift,xclimo_val_shift\n",
    "Ytrain = ytrain_shift.values\n",
    "del ytrain_shift\n",
    "Yval = yval_shift.values\n",
    "del yval_shift"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "56bbd262-3ba4-44f5-b4a1-1f3f60a104f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Prep training and validation for ANN\n",
    "config = get_config(\"exp_1\")\n",
    "training_data = CustomDataset(Xtrain,Ytrain) #xtrain, ytrain need to be numpy, not xarray\n",
    "val_data = CustomDataset(Xval,Yval)\n",
    "\n",
    "#[batch_size,channels,lat,lon]\n",
    "train_dataloader = DataLoader(training_data,batch_size = config[\"data_loader\"][\"batch_size\"],shuffle=True)\n",
    "val_dataloader  = DataLoader(val_data,batch_size = config[\"data_loader\"][\"batch_size\"],shuffle=True)\n",
    "\n",
    "# check shapes\n",
    "# for input, output in training_data:\n",
    "#     print(np.shape(input))\n",
    "#     print(np.shape(output))\n",
    "#     break\n",
    "\n",
    "# train_features, train_labels = next(iter(train_dataloader))\n",
    "# print(np.shape(train_features))\n",
    "# print(np.shape(train_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "11a40d16-8128-4396-9eed-cd274813ff00",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device available : True\n",
      "device count:  1\n",
      "current device:  0\n",
      "device name:  NVIDIA A100-SXM4-80GB\n",
      "NVIDIA A100-SXM4-80GB\n",
      "Memory Usage:\n",
      "Allocated: 5.2 GB\n",
      "Cached:    10.4 GB\n",
      "using device:  cuda\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Failed to run torchinfo. See above stack traces for more details. Executed layers up to: [CircularPad2d: 1, Sequential: 1, Conv2d: 2, GELU: 2, Sequential: 1, Conv2d: 2, GELU: 2, MaxPool2d: 1, Sequential: 1, Conv2d: 2, GELU: 2, Sequential: 1, Conv2d: 2, GELU: 2, MaxPool2d: 1, Sequential: 1, Conv2d: 2, GELU: 2, Sequential: 1, ConvTranspose2d: 2, GELU: 2, Sequential: 1, Conv2d: 2, GELU: 2]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "File \u001b[0;32m/glade/work/wchapman/miniconda3.1/envs/MLWPS/lib/python3.11/site-packages/torchinfo/torchinfo.py:295\u001b[0m, in \u001b[0;36mforward_pass\u001b[0;34m(model, x, batch_dim, cache_forward_pass, device, mode, **kwargs)\u001b[0m\n\u001b[1;32m    294\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(x, (\u001b[38;5;28mlist\u001b[39m, \u001b[38;5;28mtuple\u001b[39m)):\n\u001b[0;32m--> 295\u001b[0m     _ \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    296\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(x, \u001b[38;5;28mdict\u001b[39m):\n",
      "File \u001b[0;32m/glade/work/wchapman/miniconda3.1/envs/MLWPS/lib/python3.11/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/glade/work/wchapman/miniconda3.1/envs/MLWPS/lib/python3.11/site-packages/torch/nn/modules/module.py:1568\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1566\u001b[0m     args \u001b[38;5;241m=\u001b[39m bw_hook\u001b[38;5;241m.\u001b[39msetup_input_hook(args)\n\u001b[0;32m-> 1568\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1569\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks:\n",
      "File \u001b[0;32m/glade/work/kjmayer/research/catalyst/S2S_ocn_lnd_atm/model/train_utils.py:201\u001b[0m, in \u001b[0;36mUNet.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    199\u001b[0m cat1 \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat([conv22, upconv11], \u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m--> 201\u001b[0m up2 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mupconv2\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcat1\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;66;03m#48,77\u001b[39;00m\n\u001b[1;32m    202\u001b[0m upconv22 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mupconv22(up2)\n",
      "File \u001b[0;32m/glade/work/wchapman/miniconda3.1/envs/MLWPS/lib/python3.11/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/glade/work/wchapman/miniconda3.1/envs/MLWPS/lib/python3.11/site-packages/torch/nn/modules/module.py:1568\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1566\u001b[0m     args \u001b[38;5;241m=\u001b[39m bw_hook\u001b[38;5;241m.\u001b[39msetup_input_hook(args)\n\u001b[0;32m-> 1568\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1569\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks:\n",
      "File \u001b[0;32m/glade/work/wchapman/miniconda3.1/envs/MLWPS/lib/python3.11/site-packages/torch/nn/modules/container.py:215\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    214\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[0;32m--> 215\u001b[0m     \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    216\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[0;32m/glade/work/wchapman/miniconda3.1/envs/MLWPS/lib/python3.11/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/glade/work/wchapman/miniconda3.1/envs/MLWPS/lib/python3.11/site-packages/torch/nn/modules/module.py:1568\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1566\u001b[0m     args \u001b[38;5;241m=\u001b[39m bw_hook\u001b[38;5;241m.\u001b[39msetup_input_hook(args)\n\u001b[0;32m-> 1568\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1569\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks:\n",
      "File \u001b[0;32m/glade/work/wchapman/miniconda3.1/envs/MLWPS/lib/python3.11/site-packages/torch/nn/modules/conv.py:952\u001b[0m, in \u001b[0;36mConvTranspose2d.forward\u001b[0;34m(self, input, output_size)\u001b[0m\n\u001b[1;32m    948\u001b[0m output_padding \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_output_padding(\n\u001b[1;32m    949\u001b[0m     \u001b[38;5;28minput\u001b[39m, output_size, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstride, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mkernel_size,  \u001b[38;5;66;03m# type: ignore[arg-type]\u001b[39;00m\n\u001b[1;32m    950\u001b[0m     num_spatial_dims, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdilation)  \u001b[38;5;66;03m# type: ignore[arg-type]\u001b[39;00m\n\u001b[0;32m--> 952\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv_transpose2d\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    953\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    954\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_padding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgroups\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdilation\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: output padding must be smaller than either stride or dilation, but got output_padding_height: 2 output_padding_width: 1 stride_height: 2 stride_width: 2 dilation_height: 1 dilation_width: 1",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[55], line 8\u001b[0m\n\u001b[1;32m      5\u001b[0m model \u001b[38;5;241m=\u001b[39m UNet(config\u001b[38;5;241m=\u001b[39mconfig[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124munet_arch_atm\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m# Visualize the model\u001b[39;00m\n\u001b[0;32m----> 8\u001b[0m \u001b[43mtorchinfo\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msummary\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdata_loader\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mbatch_size\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m96\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m144\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     11\u001b[0m \u001b[43m    \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     12\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcol_names\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43minput_size\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43moutput_size\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mnum_params\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     13\u001b[0m \u001b[43m)\u001b[49m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;66;03m## grab optimizer and loss \u001b[39;00m\n\u001b[1;32m     16\u001b[0m optimizer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(torch\u001b[38;5;241m.\u001b[39moptim, config[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moptimizer\u001b[39m\u001b[38;5;124m\"\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtype\u001b[39m\u001b[38;5;124m\"\u001b[39m])(\n\u001b[1;32m     17\u001b[0m     model\u001b[38;5;241m.\u001b[39mparameters(), \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mconfig[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moptimizer\u001b[39m\u001b[38;5;124m\"\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124margs\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m     18\u001b[0m )\n",
      "File \u001b[0;32m/glade/work/wchapman/miniconda3.1/envs/MLWPS/lib/python3.11/site-packages/torchinfo/torchinfo.py:223\u001b[0m, in \u001b[0;36msummary\u001b[0;34m(model, input_size, input_data, batch_dim, cache_forward_pass, col_names, col_width, depth, device, dtypes, mode, row_settings, verbose, **kwargs)\u001b[0m\n\u001b[1;32m    216\u001b[0m validate_user_params(\n\u001b[1;32m    217\u001b[0m     input_data, input_size, columns, col_width, device, dtypes, verbose\n\u001b[1;32m    218\u001b[0m )\n\u001b[1;32m    220\u001b[0m x, correct_input_size \u001b[38;5;241m=\u001b[39m process_input(\n\u001b[1;32m    221\u001b[0m     input_data, input_size, batch_dim, device, dtypes\n\u001b[1;32m    222\u001b[0m )\n\u001b[0;32m--> 223\u001b[0m summary_list \u001b[38;5;241m=\u001b[39m \u001b[43mforward_pass\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    224\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_dim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcache_forward_pass\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel_mode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    225\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    226\u001b[0m formatting \u001b[38;5;241m=\u001b[39m FormattingOptions(depth, verbose, columns, col_width, rows)\n\u001b[1;32m    227\u001b[0m results \u001b[38;5;241m=\u001b[39m ModelStatistics(\n\u001b[1;32m    228\u001b[0m     summary_list, correct_input_size, get_total_memory_used(x), formatting\n\u001b[1;32m    229\u001b[0m )\n",
      "File \u001b[0;32m/glade/work/wchapman/miniconda3.1/envs/MLWPS/lib/python3.11/site-packages/torchinfo/torchinfo.py:304\u001b[0m, in \u001b[0;36mforward_pass\u001b[0;34m(model, x, batch_dim, cache_forward_pass, device, mode, **kwargs)\u001b[0m\n\u001b[1;32m    302\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    303\u001b[0m     executed_layers \u001b[38;5;241m=\u001b[39m [layer \u001b[38;5;28;01mfor\u001b[39;00m layer \u001b[38;5;129;01min\u001b[39;00m summary_list \u001b[38;5;28;01mif\u001b[39;00m layer\u001b[38;5;241m.\u001b[39mexecuted]\n\u001b[0;32m--> 304\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m    305\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFailed to run torchinfo. See above stack traces for more details. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    306\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExecuted layers up to: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mexecuted_layers\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    307\u001b[0m     ) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n\u001b[1;32m    308\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    309\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m hooks:\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Failed to run torchinfo. See above stack traces for more details. Executed layers up to: [CircularPad2d: 1, Sequential: 1, Conv2d: 2, GELU: 2, Sequential: 1, Conv2d: 2, GELU: 2, MaxPool2d: 1, Sequential: 1, Conv2d: 2, GELU: 2, Sequential: 1, Conv2d: 2, GELU: 2, MaxPool2d: 1, Sequential: 1, Conv2d: 2, GELU: 2, Sequential: 1, ConvTranspose2d: 2, GELU: 2, Sequential: 1, Conv2d: 2, GELU: 2]"
     ]
    }
   ],
   "source": [
    "## create NN architecture\n",
    "\n",
    "#### !!!! Look into defining groups in ConvTranspose2d and Conv2d !!!!\n",
    "\n",
    "\n",
    "config = get_config(\"exp_1\")\n",
    "# print(config[\"arch_atm\"])\n",
    "device = prepare_device(config[\"device\"])\n",
    "model = UNet(config=config[\"unet_arch_atm\"])\n",
    "\n",
    "# Visualize the model\n",
    "torchinfo.summary(\n",
    "    model,\n",
    "    input_size=(config[\"data_loader\"][\"batch_size\"], 2, 96, 144),\n",
    "    verbose=1,\n",
    "    col_names=(\"input_size\", \"output_size\", \"num_params\"),\n",
    ")\n",
    "\n",
    "## grab optimizer and loss \n",
    "optimizer = getattr(torch.optim, config[\"optimizer\"][\"type\"])(\n",
    "    model.parameters(), **config[\"optimizer\"][\"args\"]\n",
    ")\n",
    "# use yval.lat so that the lat dimension is the same as latxlon\n",
    "weights = np.cos(np.deg2rad(yval.lat)).values\n",
    "weights = weights[:,np.newaxis]\n",
    "latitude_weights = torch.tensor(weights, device=device)\n",
    "#plt.imshow(np.cos(np.deg2rad(yval.lat)).unstack('l'))\n",
    "criterion = getattr(module_metric, config[\"criterion\"])(latitude_weights)\n",
    "#criterion = getattr(torch.nn, config[\"criterion\"])() # not custom\n",
    "\n",
    "metric_funcs = [getattr(module_metric, met) for met in config[\"metrics\"]]\n",
    "\n",
    "## Build the trainer\n",
    "trainer = Trainer(\n",
    "    model,\n",
    "    criterion,\n",
    "    metric_funcs,\n",
    "    optimizer,\n",
    "    max_epochs=config[\"trainer\"][\"max_epochs\"],\n",
    "    data=train_dataloader,\n",
    "    validation_data=val_dataloader,\n",
    "    device=device,\n",
    "    config=config,\n",
    ")\n",
    "\n",
    "#Train the Model\n",
    "model.to(device)\n",
    "trainer.fit()\n",
    "\n",
    "## Plot training:\n",
    "print(trainer.log.history.keys())\n",
    "plt.figure(figsize=(20, 4))\n",
    "for i, m in enumerate((\"loss\", *config[\"metrics\"])):\n",
    "    plt.subplot(1, 4, i + 1)\n",
    "    plt.plot(trainer.log.history[\"epoch\"], trainer.log.history[m], label=m)\n",
    "    plt.plot(trainer.log.history[\"epoch\"], trainer.log.history[\"val_\" + m], label=\"val_\" + m)\n",
    "    plt.axvline(x=trainer.early_stopper.best_epoch, linestyle=\"--\", color=\"k\", linewidth=0.75)\n",
    "    plt.title(m)\n",
    "    plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a9d02f2-5e38-4fdd-baef-254e5ba09ed9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# val_pred = model.predict(dataset=val_data, batch_size=config[\"data_loader\"][\"batch_size\"], device=device)\n",
    "val_pred = model.predict(dataloader=val_dataloader, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cfaa70e-a81f-43e5-8882-a80f64cb6b9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in np.arange(0,300,50):\n",
    "    val_pred_reshape = val_pred[i].reshape((96,144))\n",
    "    plt.imshow(val_pred_reshape,origin='lower',cmap='RdBu_r',vmin=0,vmax=1)\n",
    "    plt.show()\n",
    "    # Yval_reshape = Yval[i].reshape((96,144))\n",
    "    # plt.imshow(Yval_reshape,origin='lower',cmap='RdBu_r',vmin=0,vmax=1)\n",
    "    # plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67dbe141-845a-4990-bb92-9053c7871dff",
   "metadata": {},
   "source": [
    "## Other Plots:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f9390fea-a989-4b8f-93c0-3d7e49ed2bb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i in np.arange(0,300,10):\n",
    "#     Yval_reshape = Yval[i].reshape((17,144))\n",
    "#     plt.imshow(Yval_reshape,origin='lower',cmap='RdBu_r',vmin=0,vmax=1)\n",
    "#     # plt.colorbar(orientation='horizontal')\n",
    "#     plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "287aac18-f62b-48a7-a27a-cc874a581c09",
   "metadata": {},
   "outputs": [],
   "source": [
    "# isample = 600\n",
    "# ytrainmax = xtrainmax.sel(lat=slice(29,61)).values\n",
    "# ytrainmin = xtrainmin.sel(lat=slice(29,61)).values\n",
    "\n",
    "# val_pred_reshape = val_pred[isample].reshape((17,144))\n",
    "# val_pred_denorm = (val_pred_reshape*(ytrainmax-ytrainmin)) + ytrainmin\n",
    "\n",
    "# Yval_reshape = Yval[isample].reshape((17,144))\n",
    "# Yval_denorm = (Yval_reshape*(ytrainmax-ytrainmin)) + ytrainmin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ca35cc32-3d95-4c76-b32f-f6cf3dcebe60",
   "metadata": {},
   "outputs": [],
   "source": [
    "# compare\n",
    "# ydiff = val_pred_reshape - Yval_reshape\n",
    "# plt.imshow(ydiff,origin='lower',cmap='RdBu_r')\n",
    "# plt.colorbar(orientation='horizontal')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e845daaf-e823-4a08-be52-712242058cc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.imshow(val_pred_denorm,origin='lower',cmap='RdBu_r',vmin=-3,vmax=3)\n",
    "# plt.colorbar(orientation='horizontal')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1afee442-4eae-43e3-ab01-583819b030d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.imshow(Yval_denorm,origin='lower',cmap='RdBu_r',vmin=-3,vmax=3)\n",
    "# plt.colorbar(orientation='horizontal')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8eeb77b2-0308-47df-9f39-b6edaa7aa7d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ydiff_denorm = val_pred_denorm - Yval_denorm\n",
    "# plt.imshow(ydiff_denorm,origin='lower',cmap='RdBu_r')\n",
    "# plt.colorbar(orientation='horizontal')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f6bdd7a-716c-4c99-8933-cb175550c470",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MLWPS",
   "language": "python",
   "name": "mlwps"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
