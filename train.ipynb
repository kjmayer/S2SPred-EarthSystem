{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "186b954f-dd38-495a-a5e7-30b0cced502b",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_dir = '/glade/work/kjmayer/research/catalyst/S2S_ocn_lnd_atm/'\n",
    "import xarray as xr\n",
    "import numpy as np\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torchinfo\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "from model.train_utils import UNet\n",
    "import utils.utils\n",
    "from utils.utils import get_config\n",
    "from utils.utils import prepare_device\n",
    "from trainer.trainer import Trainer\n",
    "import model.metrics as module_metric\n",
    "from data_prep.data_loader import GetXData, GetYData, CustomDataset,lead_shift, concat_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9b1c9670-cf98-4379-bf1c-0cc3be7bcb80",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'NVIDIA A100-SXM4-80GB'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# torch.cuda.is_available()\n",
    "# torch.cuda.device_count()\n",
    "# torch.cuda.current_device()\n",
    "# torch.cuda.device(0)\n",
    "torch.cuda.get_device_name(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "56953f61-bc6e-477e-bcce-6a5cf23d21f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = get_config(\"exp_1\")\n",
    "\n",
    "torch.manual_seed(config[\"seed\"])\n",
    "torch.cuda.manual_seed(config[\"seed\"])\n",
    "np.random.seed(config[\"seed\"])\n",
    "random.seed(config[\"seed\"])\n",
    "torch.backends.cudnn.deterministic = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3e0c151f-25a5-4d4c-ad70-d1f3ddd432a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/glade/work/wchapman/miniconda3.1/envs/MLWPS/lib/python3.11/site-packages/xarray/core/variable.py:1546: PerformanceWarning: Reshaping is producing a large chunk. To accept the large\n",
      "chunk and silence this warning, set the option\n",
      "    >>> with dask.config.set(**{'array.slicing.split_large_chunks': False}):\n",
      "    ...     array.reshape(shape)\n",
      "\n",
      "To avoid creating the large chunks, set the option\n",
      "    >>> with dask.config.set(**{'array.slicing.split_large_chunks': True}):\n",
      "    ...     array.reshape(shape)Explictly passing ``limit`` to ``reshape`` will also silence this warning\n",
      "    >>> array.reshape(shape, limit='128 MiB')\n",
      "  result = result._stack_once(dims, new_dim)\n",
      "/glade/work/wchapman/miniconda3.1/envs/MLWPS/lib/python3.11/site-packages/xarray/core/variable.py:1546: PerformanceWarning: Reshaping is producing a large chunk. To accept the large\n",
      "chunk and silence this warning, set the option\n",
      "    >>> with dask.config.set(**{'array.slicing.split_large_chunks': False}):\n",
      "    ...     array.reshape(shape)\n",
      "\n",
      "To avoid creating the large chunks, set the option\n",
      "    >>> with dask.config.set(**{'array.slicing.split_large_chunks': True}):\n",
      "    ...     array.reshape(shape)Explictly passing ``limit`` to ``reshape`` will also silence this warning\n",
      "    >>> array.reshape(shape, limit='128 MiB')\n",
      "  result = result._stack_once(dims, new_dim)\n",
      "/glade/work/wchapman/miniconda3.1/envs/MLWPS/lib/python3.11/site-packages/xarray/core/variable.py:1546: PerformanceWarning: Reshaping is producing a large chunk. To accept the large\n",
      "chunk and silence this warning, set the option\n",
      "    >>> with dask.config.set(**{'array.slicing.split_large_chunks': False}):\n",
      "    ...     array.reshape(shape)\n",
      "\n",
      "To avoid creating the large chunks, set the option\n",
      "    >>> with dask.config.set(**{'array.slicing.split_large_chunks': True}):\n",
      "    ...     array.reshape(shape)Explictly passing ``limit`` to ``reshape`` will also silence this warning\n",
      "    >>> array.reshape(shape, limit='128 MiB')\n",
      "  result = result._stack_once(dims, new_dim)\n",
      "/glade/work/wchapman/miniconda3.1/envs/MLWPS/lib/python3.11/site-packages/xarray/core/variable.py:1546: PerformanceWarning: Reshaping is producing a large chunk. To accept the large\n",
      "chunk and silence this warning, set the option\n",
      "    >>> with dask.config.set(**{'array.slicing.split_large_chunks': False}):\n",
      "    ...     array.reshape(shape)\n",
      "\n",
      "To avoid creating the large chunks, set the option\n",
      "    >>> with dask.config.set(**{'array.slicing.split_large_chunks': True}):\n",
      "    ...     array.reshape(shape)Explictly passing ``limit`` to ``reshape`` will also silence this warning\n",
      "    >>> array.reshape(shape, limit='128 MiB')\n",
      "  result = result._stack_once(dims, new_dim)\n"
     ]
    }
   ],
   "source": [
    "LEAD = config[\"data_loader\"][\"lead\"][0] # will loop over this for training eventually\n",
    "trainfinames = config[\"data_loader\"][\"anommems_finames\"][0:6]\n",
    "valfinames = config[\"data_loader\"][\"anommems_finames\"][6:8]\n",
    "# xtrain, xtrainmean, xtrainstd, xtrainmin, xtrainmax\n",
    "xtrain, xtrainmin, xtrainmax = GetXData(dir=config[\"data_loader\"][\"base_dir\"],\n",
    "                                      var=config[\"data_loader\"][\"atm_var\"],\n",
    "                                      finames=trainfinames,\n",
    "                                      train=True,\n",
    "                                      climo=False)[0]\n",
    "# xtrain = xtrain.stack(l=('lat','lon'))\n",
    "xtrain_shift = lead_shift(xtrain, lead=LEAD, forward=False)\n",
    "\n",
    "#ytrain, ytrainmean, ytrainstd, ytrainmax, ytrainmin\n",
    "ytrain, ytrainmax, ytrainmin = GetYData(dir=config[\"data_loader\"][\"base_dir\"],\n",
    "                                       var=config[\"data_loader\"][\"atm_var\"],\n",
    "                                       finames=trainfinames,\n",
    "                                       train=True,\n",
    "                                       norm=True,\n",
    "                                       )[0]\n",
    "\n",
    "# ytrain = ytrain.sel(lat=slice(29,61))\n",
    "# ytrain = ytrain.stack(l=('lat','lon'))\n",
    "ytrain_shift = lead_shift(ytrain, lead=LEAD, forward=True)\n",
    "\n",
    "xval = GetXData(dir=config[\"data_loader\"][\"base_dir\"],\n",
    "                var=config[\"data_loader\"][\"atm_var\"],\n",
    "                finames=valfinames,\n",
    "                train=False,\n",
    "                # trainmean=xtrainmean,\n",
    "                # trainstd=xtrainstd,\n",
    "                trainmin=xtrainmin,\n",
    "                trainmax=xtrainmax,\n",
    "                climo=False)[0]\n",
    "# \n",
    "xval_shift = lead_shift(xval, lead=LEAD, forward=False)\n",
    "\n",
    "yval = GetYData(dir=config[\"data_loader\"][\"base_dir\"],\n",
    "                var=config[\"data_loader\"][\"atm_var\"],\n",
    "                finames=trainfinames,\n",
    "                train=False,\n",
    "                # trainmean=ytrainmean,\n",
    "                # trainstd=ytrainstd,\n",
    "                norm=True,\n",
    "                trainmax=ytrainmax,\n",
    "                trainmin=ytrainmin,\n",
    "                )[0]\n",
    "\n",
    "# yval = yval.sel(lat=slice(29,61))\n",
    "# yval = yval.stack(l=('lat','lon'))\n",
    "yval_shift = lead_shift(yval, lead=LEAD, forward=True)\n",
    "\n",
    "\n",
    "# climo (same for train, val, and test --> basically a DOY encoder)\n",
    "xclimo, climomin, climomax = GetXData(dir = config[\"data_loader\"][\"base_dir\"],\n",
    "                                     var = config[\"data_loader\"][\"atm_var\"],\n",
    "                                     finames = config[\"data_loader\"][\"climo_finame\"],\n",
    "                                     train = False, # MUST ALWAYS BE False FOR CLIMO\n",
    "                                     climo = True)[0]\n",
    "# climo appended to same length as training\n",
    "xclimo = xclimo.rename({'dayofyear': 'time'})\n",
    "xclimo_train = xr.concat([xclimo]*int(xtrain.shape[0]/365),dim='mem')\n",
    "xclimo_train = xclimo_train.stack(s=('mem','time')).transpose('s', 'lat', 'lon').reset_index(['s'])\n",
    "xclimo_train_shift = lead_shift(xclimo_train,lead=LEAD,forward=False)\n",
    "\n",
    "# climo appended to same length as validation\n",
    "xclimo_val = xr.concat([xclimo]*int(xval.shape[0]/365),dim='mem')\n",
    "xclimo_val = xclimo_val.stack(s=('mem','time')).transpose('s', 'lat', 'lon').reset_index(['s'])\n",
    "xclimo_val_shift = lead_shift(xclimo_val,lead=LEAD,forward=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fea1995c-49af-4621-8b7f-3b7a10d2659a",
   "metadata": {},
   "outputs": [],
   "source": [
    "Xtrain = concat_input(xtrain_shift,xclimo_train_shift,dim_name='features').values\n",
    "# Xtrain = xtrain_shift.expand_dims(dim={\"features\": 1},axis=1).values\n",
    "del xtrain_shift,xclimo_train_shift\n",
    "Xval = concat_input(xval_shift,xclimo_val_shift,dim_name='features').values\n",
    "# Xval = xval_shift.expand_dims(dim={\"features\": 1},axis=1).values\n",
    "del xval_shift,xclimo_val_shift\n",
    "Ytrain = ytrain_shift.values\n",
    "del ytrain_shift\n",
    "Yval = yval_shift.values\n",
    "del yval_shift"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "56bbd262-3ba4-44f5-b4a1-1f3f60a104f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Prep training and validation for ANN\n",
    "config = get_config(\"exp_1\")\n",
    "training_data = CustomDataset(Xtrain,Ytrain) #xtrain, ytrain need to be numpy, not xarray\n",
    "val_data = CustomDataset(Xval,Yval)\n",
    "\n",
    "#[batch_size,channels,lat,lon]\n",
    "train_dataloader = DataLoader(training_data,batch_size = config[\"data_loader\"][\"batch_size\"],shuffle=True)\n",
    "val_dataloader  = DataLoader(val_data,batch_size = config[\"data_loader\"][\"batch_size\"],shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8beea90f-9f06-4079-960f-7bacc718d7c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 2, 96, 144])\n",
      "torch.Size([32, 96, 144])\n"
     ]
    }
   ],
   "source": [
    "# check shapes\n",
    "# for input, output in training_data:\n",
    "#     print(np.shape(input))\n",
    "#     print(np.shape(output))\n",
    "#     break\n",
    "\n",
    "train_features, train_labels = next(iter(train_dataloader))\n",
    "print(np.shape(train_features))\n",
    "print(np.shape(train_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "11a40d16-8128-4396-9eed-cd274813ff00",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device available : True\n",
      "device count:  1\n",
      "current device:  0\n",
      "device name:  NVIDIA A100-SXM4-80GB\n",
      "NVIDIA A100-SXM4-80GB\n",
      "Memory Usage:\n",
      "Allocated: 0.0 GB\n",
      "Cached:    0.8 GB\n",
      "using device:  cuda\n",
      "===================================================================================================================\n",
      "Layer (type:depth-idx)                   Input Shape               Output Shape              Param #\n",
      "===================================================================================================================\n",
      "UNet                                     [32, 2, 96, 144]          [32, 1, 96, 144]          --\n",
      "├─CircularPad2d: 1-1                     [32, 2, 96, 144]          [32, 2, 96, 154]          --\n",
      "├─Sequential: 1-2                        [32, 2, 96, 154]          [32, 32, 96, 154]         --\n",
      "│    └─Conv2d: 2-1                       [32, 2, 96, 154]          [32, 32, 96, 154]         1,632\n",
      "│    └─ReLU: 2-2                         [32, 32, 96, 154]         [32, 32, 96, 154]         --\n",
      "├─MaxPool2d: 1-3                         [32, 32, 96, 154]         [32, 32, 48, 77]          --\n",
      "├─Sequential: 1-4                        [32, 32, 48, 77]          [32, 128, 48, 77]         --\n",
      "│    └─Conv2d: 2-3                       [32, 32, 48, 77]          [32, 128, 48, 77]         102,528\n",
      "│    └─ReLU: 2-4                         [32, 128, 48, 77]         [32, 128, 48, 77]         --\n",
      "├─MaxPool2d: 1-5                         [32, 128, 48, 77]         [32, 128, 24, 39]         --\n",
      "├─Sequential: 1-6                        [32, 128, 24, 39]         [32, 128, 48, 77]         --\n",
      "│    └─ConvTranspose2d: 2-5              [32, 128, 24, 39]         [32, 128, 48, 77]         409,728\n",
      "│    └─ReLU: 2-6                         [32, 128, 48, 77]         [32, 128, 48, 77]         --\n",
      "├─Sequential: 1-7                        [32, 256, 48, 77]         [32, 32, 96, 154]         --\n",
      "│    └─ConvTranspose2d: 2-7              [32, 256, 48, 77]         [32, 32, 96, 154]         73,760\n",
      "│    └─ReLU: 2-8                         [32, 32, 96, 154]         [32, 32, 96, 154]         --\n",
      "├─Sequential: 1-8                        [32, 64, 96, 154]         [32, 1, 96, 154]          --\n",
      "│    └─ConvTranspose2d: 2-9              [32, 64, 96, 154]         [32, 1, 96, 154]          577\n",
      "===================================================================================================================\n",
      "Total params: 588,225\n",
      "Trainable params: 588,225\n",
      "Non-trainable params: 0\n",
      "Total mult-adds (Units.GIGABYTES): 96.53\n",
      "===================================================================================================================\n",
      "Input size (MB): 3.54\n",
      "Forward/backward pass size (MB): 488.23\n",
      "Params size (MB): 2.35\n",
      "Estimated Total Size (MB): 494.12\n",
      "===================================================================================================================\n",
      "Epoch   0/30\n",
      "  104.6s - train_loss: 0.07639 - val_loss: 0.06634\n",
      "Epoch   1/30\n",
      "  71.6s - train_loss: 0.06446 - val_loss: 0.06494\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[18], line 44\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[38;5;66;03m#Train the Model\u001b[39;00m\n\u001b[1;32m     43\u001b[0m model\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m---> 44\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/glade/work/kjmayer/research/catalyst/S2S_ocn_lnd_atm/base/base_trainer.py:68\u001b[0m, in \u001b[0;36mBaseTrainer.fit\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     64\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmax_epochs \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m):\n\u001b[1;32m     66\u001b[0m     start_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[0;32m---> 68\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_train_epoch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mepoch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     70\u001b[0m     \u001b[38;5;66;03m# log the results of the epoch\u001b[39;00m\n\u001b[1;32m     71\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbatch_log\u001b[38;5;241m.\u001b[39mresult()\n",
      "File \u001b[0;32m/glade/work/kjmayer/research/catalyst/S2S_ocn_lnd_atm/trainer/trainer.py:81\u001b[0m, in \u001b[0;36mTrainer._train_epoch\u001b[0;34m(self, epoch)\u001b[0m\n\u001b[1;32m     79\u001b[0m \u001b[38;5;66;03m# Log the results\u001b[39;00m\n\u001b[1;32m     80\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbatch_log\u001b[38;5;241m.\u001b[39mupdate(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbatch\u001b[39m\u001b[38;5;124m\"\u001b[39m, batch_idx)\n\u001b[0;32m---> 81\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbatch_log\u001b[38;5;241m.\u001b[39mupdate(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mloss\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m     82\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m met \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmetric_funcs:\n\u001b[1;32m     83\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbatch_log\u001b[38;5;241m.\u001b[39mupdate(met\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, met(output, target))\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "## create NN architecture\n",
    "config = get_config(\"exp_1\")\n",
    "# print(config[\"arch_atm\"])\n",
    "device = prepare_device(config[\"device\"])\n",
    "model = UNet(config=config[\"unet_arch_atm\"])\n",
    "\n",
    "## grab optimizer and loss \n",
    "optimizer = getattr(torch.optim, config[\"optimizer\"][\"type\"])(\n",
    "    model.parameters(), **config[\"optimizer\"][\"args\"]\n",
    ")\n",
    "# use yval.lat so that the lat dimension is the same as latxlon\n",
    "weights = np.cos(np.deg2rad(yval.lat)).values\n",
    "weights = weights[:,np.newaxis]\n",
    "latitude_weights = torch.tensor(weights, device=device)\n",
    "#plt.imshow(np.cos(np.deg2rad(yval.lat)).unstack('l'))\n",
    "criterion = getattr(module_metric, config[\"criterion\"])(latitude_weights)\n",
    "#criterion = getattr(torch.nn, config[\"criterion\"])() # not custom\n",
    "\n",
    "metric_funcs = [getattr(module_metric, met) for met in config[\"metrics\"]]\n",
    "\n",
    "## Build the trainer\n",
    "trainer = Trainer(\n",
    "    model,\n",
    "    criterion,\n",
    "    metric_funcs,\n",
    "    optimizer,\n",
    "    max_epochs=config[\"trainer\"][\"max_epochs\"],\n",
    "    data=train_dataloader,\n",
    "    validation_data=val_dataloader,\n",
    "    device=device,\n",
    "    config=config,\n",
    ")\n",
    "\n",
    "# Visualize the model\n",
    "torchinfo.summary(\n",
    "    model,\n",
    "    input_size=(config[\"data_loader\"][\"batch_size\"], 2, 96, 144),\n",
    "    verbose=1,\n",
    "    col_names=(\"input_size\", \"output_size\", \"num_params\"),\n",
    ")\n",
    "\n",
    "#Train the Model\n",
    "model.to(device)\n",
    "trainer.fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a2b8773-4894-4e8c-97fb-5c6ee580933e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(trainer.log.history.keys())\n",
    "\n",
    "plt.figure(figsize=(20, 4))\n",
    "for i, m in enumerate((\"loss\", *config[\"metrics\"])):\n",
    "    plt.subplot(1, 4, i + 1)\n",
    "    plt.plot(trainer.log.history[\"epoch\"], trainer.log.history[m], label=m)\n",
    "    plt.plot(\n",
    "        trainer.log.history[\"epoch\"], trainer.log.history[\"val_\" + m], label=\"val_\" + m\n",
    "    )\n",
    "    plt.axvline(\n",
    "        x=trainer.early_stopper.best_epoch, linestyle=\"--\", color=\"k\", linewidth=0.75\n",
    "    )\n",
    "    plt.title(m)\n",
    "    plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a9d02f2-5e38-4fdd-baef-254e5ba09ed9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# val_pred = model.predict(dataset=val_data, batch_size=config[\"data_loader\"][\"batch_size\"], device=device)\n",
    "val_pred = model.predict(dataloader=val_dataloader, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cfaa70e-a81f-43e5-8882-a80f64cb6b9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in np.arange(0,300,10):\n",
    "    val_pred_reshape = val_pred[i].reshape((96,144))\n",
    "    plt.imshow(val_pred_reshape,origin='lower',cmap='RdBu_r',vmin=0,vmax=1)\n",
    "    plt.show()\n",
    "    # Yval_reshape = Yval[i].reshape((96,144))\n",
    "    # plt.imshow(Yval_reshape,origin='lower',cmap='RdBu_r',vmin=0,vmax=1)\n",
    "    # plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f9390fea-a989-4b8f-93c0-3d7e49ed2bb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i in np.arange(0,300,10):\n",
    "#     Yval_reshape = Yval[i].reshape((17,144))\n",
    "#     plt.imshow(Yval_reshape,origin='lower',cmap='RdBu_r',vmin=0,vmax=1)\n",
    "#     # plt.colorbar(orientation='horizontal')\n",
    "#     plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "287aac18-f62b-48a7-a27a-cc874a581c09",
   "metadata": {},
   "outputs": [],
   "source": [
    "# isample = 600\n",
    "# ytrainmax = xtrainmax.sel(lat=slice(29,61)).values\n",
    "# ytrainmin = xtrainmin.sel(lat=slice(29,61)).values\n",
    "\n",
    "# val_pred_reshape = val_pred[isample].reshape((17,144))\n",
    "# val_pred_denorm = (val_pred_reshape*(ytrainmax-ytrainmin)) + ytrainmin\n",
    "\n",
    "# Yval_reshape = Yval[isample].reshape((17,144))\n",
    "# Yval_denorm = (Yval_reshape*(ytrainmax-ytrainmin)) + ytrainmin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ca35cc32-3d95-4c76-b32f-f6cf3dcebe60",
   "metadata": {},
   "outputs": [],
   "source": [
    "# compare\n",
    "# ydiff = val_pred_reshape - Yval_reshape\n",
    "# plt.imshow(ydiff,origin='lower',cmap='RdBu_r')\n",
    "# plt.colorbar(orientation='horizontal')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e845daaf-e823-4a08-be52-712242058cc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.imshow(val_pred_denorm,origin='lower',cmap='RdBu_r',vmin=-3,vmax=3)\n",
    "# plt.colorbar(orientation='horizontal')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1afee442-4eae-43e3-ab01-583819b030d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.imshow(Yval_denorm,origin='lower',cmap='RdBu_r',vmin=-3,vmax=3)\n",
    "# plt.colorbar(orientation='horizontal')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8eeb77b2-0308-47df-9f39-b6edaa7aa7d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ydiff_denorm = val_pred_denorm - Yval_denorm\n",
    "# plt.imshow(ydiff_denorm,origin='lower',cmap='RdBu_r')\n",
    "# plt.colorbar(orientation='horizontal')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f6bdd7a-716c-4c99-8933-cb175550c470",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MLWPS",
   "language": "python",
   "name": "mlwps"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
