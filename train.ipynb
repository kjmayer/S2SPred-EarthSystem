{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "959cb75a-32de-4d34-bcdc-4687a1c9f155",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_dir = '/glade/work/kjmayer/research/catalyst/S2S_ocn_lnd_atm/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "186b954f-dd38-495a-a5e7-30b0cced502b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import xarray as xr\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "import torch\n",
    "import torchinfo\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "from model.train_utils import NeuralNetwork\n",
    "import utils.utils\n",
    "from utils.utils import get_config\n",
    "from utils.utils import prepare_device\n",
    "from trainer.trainer import Trainer\n",
    "import model.metrics as module_metric\n",
    "from data_prep.data_loader import GetData,CustomDataset,lead_shift, concat_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9b1c9670-cf98-4379-bf1c-0cc3be7bcb80",
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "56953f61-bc6e-477e-bcce-6a5cf23d21f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = get_config(\"exp_1\")\n",
    "\n",
    "torch.manual_seed(config[\"seed\"])\n",
    "torch.cuda.manual_seed(config[\"seed\"])\n",
    "np.random.seed(config[\"seed\"])\n",
    "random.seed(config[\"seed\"])\n",
    "torch.backends.cudnn.deterministic = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3e0c151f-25a5-4d4c-ad70-d1f3ddd432a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/glade/work/wchapman/miniconda3.1/envs/MLWPS/lib/python3.11/site-packages/xarray/core/variable.py:1546: PerformanceWarning: Reshaping is producing a large chunk. To accept the large\n",
      "chunk and silence this warning, set the option\n",
      "    >>> with dask.config.set(**{'array.slicing.split_large_chunks': False}):\n",
      "    ...     array.reshape(shape)\n",
      "\n",
      "To avoid creating the large chunks, set the option\n",
      "    >>> with dask.config.set(**{'array.slicing.split_large_chunks': True}):\n",
      "    ...     array.reshape(shape)Explictly passing ``limit`` to ``reshape`` will also silence this warning\n",
      "    >>> array.reshape(shape, limit='128 MiB')\n",
      "  result = result._stack_once(dims, new_dim)\n",
      "/glade/work/wchapman/miniconda3.1/envs/MLWPS/lib/python3.11/site-packages/xarray/core/variable.py:1546: PerformanceWarning: Reshaping is producing a large chunk. To accept the large\n",
      "chunk and silence this warning, set the option\n",
      "    >>> with dask.config.set(**{'array.slicing.split_large_chunks': False}):\n",
      "    ...     array.reshape(shape)\n",
      "\n",
      "To avoid creating the large chunks, set the option\n",
      "    >>> with dask.config.set(**{'array.slicing.split_large_chunks': True}):\n",
      "    ...     array.reshape(shape)Explictly passing ``limit`` to ``reshape`` will also silence this warning\n",
      "    >>> array.reshape(shape, limit='128 MiB')\n",
      "  result = result._stack_once(dims, new_dim)\n",
      "/glade/work/wchapman/miniconda3.1/envs/MLWPS/lib/python3.11/site-packages/xarray/core/variable.py:1546: PerformanceWarning: Reshaping is producing a large chunk. To accept the large\n",
      "chunk and silence this warning, set the option\n",
      "    >>> with dask.config.set(**{'array.slicing.split_large_chunks': False}):\n",
      "    ...     array.reshape(shape)\n",
      "\n",
      "To avoid creating the large chunks, set the option\n",
      "    >>> with dask.config.set(**{'array.slicing.split_large_chunks': True}):\n",
      "    ...     array.reshape(shape)Explictly passing ``limit`` to ``reshape`` will also silence this warning\n",
      "    >>> array.reshape(shape, limit='128 MiB')\n",
      "  result = result._stack_once(dims, new_dim)\n",
      "/glade/work/wchapman/miniconda3.1/envs/MLWPS/lib/python3.11/site-packages/xarray/core/variable.py:1546: PerformanceWarning: Reshaping is producing a large chunk. To accept the large\n",
      "chunk and silence this warning, set the option\n",
      "    >>> with dask.config.set(**{'array.slicing.split_large_chunks': False}):\n",
      "    ...     array.reshape(shape)\n",
      "\n",
      "To avoid creating the large chunks, set the option\n",
      "    >>> with dask.config.set(**{'array.slicing.split_large_chunks': True}):\n",
      "    ...     array.reshape(shape)Explictly passing ``limit`` to ``reshape`` will also silence this warning\n",
      "    >>> array.reshape(shape, limit='128 MiB')\n",
      "  result = result._stack_once(dims, new_dim)\n"
     ]
    }
   ],
   "source": [
    "LEAD = 7 # will loop over this for training eventually\n",
    "trainfinames = config[\"data_loader\"][\"anommems_finames\"][0:6]\n",
    "valfinames = config[\"data_loader\"][\"anommems_finames\"][6:8]\n",
    "\n",
    "xtrain, xtrainmean, xtrainstd, xtrainmin, xtrainmax = GetData(dir=config[\"data_loader\"][\"base_dir\"],\n",
    "                                                              var=config[\"data_loader\"][\"atm_var\"],\n",
    "                                                              finames=trainfinames,\n",
    "                                                              train=True,\n",
    "                                                              climo=False)[0]\n",
    "# xtrain = xtrain.stack(l=('lat','lon'))\n",
    "xtrain_shift = lead_shift(xtrain, lead=LEAD, forward=False)\n",
    "ytrain = xtrain.sel(lat=slice(29,61))\n",
    "ytrain = ytrain.stack(l=('lat','lon'))\n",
    "ytrain_shift = lead_shift(ytrain, lead=LEAD, forward=True)\n",
    "\n",
    "xval = GetData(dir=config[\"data_loader\"][\"base_dir\"],\n",
    "               var=config[\"data_loader\"][\"atm_var\"],\n",
    "               finames=valfinames,\n",
    "               train=False,\n",
    "               trainmean=xtrainmean,\n",
    "               trainstd=xtrainstd,\n",
    "               trainmin=xtrainmin,\n",
    "               trainmax=xtrainmax,\n",
    "               climo=False)[0]\n",
    "# \n",
    "xval_shift = lead_shift(xval, lead=LEAD, forward=False)\n",
    "yval = xval.sel(lat=slice(29,61))\n",
    "yval = yval.stack(l=('lat','lon'))\n",
    "yval_shift = lead_shift(yval, lead=LEAD, forward=True)\n",
    "\n",
    "\n",
    "# climo (same for train, val, and test --> basically a DOY encoder)\n",
    "xclimo, climomin, climomax = GetData(dir = config[\"data_loader\"][\"base_dir\"],\n",
    "                                     var = config[\"data_loader\"][\"atm_var\"],\n",
    "                                     finames = config[\"data_loader\"][\"climo_finame\"],\n",
    "                                     train = False, # MUST ALWAYS BE False FOR CLIMO\n",
    "                                     climo = True)[0]\n",
    "# climo appended to same length as training\n",
    "xclimo = xclimo.rename({'dayofyear': 'time'})\n",
    "xclimo_train = xr.concat([xclimo]*int(xtrain.shape[0]/365),dim='mem')\n",
    "xclimo_train = xclimo_train.stack(s=('mem','time')).transpose('s', 'lat', 'lon').reset_index(['s'])\n",
    "xclimo_train_shift = lead_shift(xclimo_train,lead=LEAD,forward=False)\n",
    "\n",
    "# climo appended to same length as validation\n",
    "xclimo_val = xr.concat([xclimo]*int(xval.shape[0]/365),dim='mem')\n",
    "xclimo_val = xclimo_val.stack(s=('mem','time')).transpose('s', 'lat', 'lon').reset_index(['s'])\n",
    "xclimo_val_shift = lead_shift(xclimo_val,lead=LEAD,forward=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fea1995c-49af-4621-8b7f-3b7a10d2659a",
   "metadata": {},
   "outputs": [],
   "source": [
    "Xtrain = concat_input(xtrain_shift,xclimo_train_shift,dim_name='features').values\n",
    "del xtrain_shift,xclimo_train_shift\n",
    "Xval = concat_input(xval_shift,xclimo_val_shift,dim_name='features').values\n",
    "del xval_shift,xclimo_val_shift\n",
    "Ytrain = ytrain_shift.values\n",
    "del ytrain_shift\n",
    "Yval = yval_shift.values\n",
    "del yval_shift"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "56bbd262-3ba4-44f5-b4a1-1f3f60a104f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Prep training and validation for ANN\n",
    "training_data = CustomDataset(Xtrain,Ytrain) #xtrain, ytrain need to be numpy, not xarray\n",
    "val_data = CustomDataset(Xval,Yval)\n",
    "\n",
    "#[batch_size,channels,lat,lon]\n",
    "train_dataloader = DataLoader(training_data,batch_size = config[\"data_loader\"][\"batch_size\"],shuffle=True)\n",
    "val_dataloader  = DataLoader(val_data,batch_size = config[\"data_loader\"][\"batch_size\"],shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8beea90f-9f06-4079-960f-7bacc718d7c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 96, 144])\n",
      "torch.Size([2448])\n",
      "torch.Size([32, 2, 96, 144])\n",
      "torch.Size([32, 2448])\n"
     ]
    }
   ],
   "source": [
    "# check shapes\n",
    "for input, output in training_data:\n",
    "    print(np.shape(input))\n",
    "    print(np.shape(output))\n",
    "    break\n",
    "\n",
    "train_features, train_labels = next(iter(train_dataloader))\n",
    "print(np.shape(train_features))\n",
    "print(np.shape(train_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11a40d16-8128-4396-9eed-cd274813ff00",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/glade/work/wchapman/miniconda3.1/envs/MLWPS/lib/python3.11/site-packages/torch/nn/modules/lazy.py:180: UserWarning: Lazy modules are a new feature under heavy development so changes to the API or functionality can happen at any moment.\n",
      "  warnings.warn('Lazy modules are a new feature under heavy development '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===================================================================================================================\n",
      "Layer (type:depth-idx)                   Input Shape               Output Shape              Param #\n",
      "===================================================================================================================\n",
      "NeuralNetwork                            [32, 2, 96, 144]          [32, 2448]                --\n",
      "├─CircularPad2d: 1-1                     [32, 2, 96, 144]          [32, 2, 96, 154]          --\n",
      "├─Sequential: 1-2                        [32, 2, 96, 154]          [32, 32, 24, 39]          --\n",
      "│    └─Sequential: 2-1                   [32, 2, 96, 154]          [32, 32, 48, 77]          --\n",
      "│    │    └─Conv2d: 3-1                  [32, 2, 96, 154]          [32, 32, 96, 154]         1,632\n",
      "│    │    └─ReLU: 3-2                    [32, 32, 96, 154]         [32, 32, 96, 154]         --\n",
      "│    │    └─MaxPool2d: 3-3               [32, 32, 96, 154]         [32, 32, 48, 77]          --\n",
      "│    └─Sequential: 2-2                   [32, 32, 48, 77]          [32, 32, 24, 39]          --\n",
      "│    │    └─Conv2d: 3-4                  [32, 32, 48, 77]          [32, 32, 48, 77]          9,248\n",
      "│    │    └─ReLU: 3-5                    [32, 32, 48, 77]          [32, 32, 48, 77]          --\n",
      "│    │    └─MaxPool2d: 3-6               [32, 32, 48, 77]          [32, 32, 24, 39]          --\n",
      "├─Flatten: 1-3                           [32, 32, 24, 39]          [32, 29952]               --\n",
      "├─Sequential: 1-4                        [32, 29952]               [32, 100]                 --\n",
      "│    └─Sequential: 2-3                   [32, 29952]               [32, 100]                 --\n",
      "│    │    └─Linear: 3-7                  [32, 29952]               [32, 100]                 2,995,300\n",
      "│    │    └─ReLU: 3-8                    [32, 100]                 [32, 100]                 --\n",
      "│    └─Sequential: 2-4                   [32, 100]                 [32, 100]                 --\n",
      "│    │    └─Linear: 3-9                  [32, 100]                 [32, 100]                 10,100\n",
      "│    │    └─ReLU: 3-10                   [32, 100]                 [32, 100]                 --\n",
      "├─Sequential: 1-5                        [32, 100]                 [32, 2448]                --\n",
      "│    └─Linear: 2-5                       [32, 100]                 [32, 2448]                247,248\n",
      "===================================================================================================================\n",
      "Total params: 3,263,528\n",
      "Trainable params: 3,263,528\n",
      "Non-trainable params: 0\n",
      "Total mult-adds (Units.GIGABYTES): 1.97\n",
      "===================================================================================================================\n",
      "Input size (MB): 3.54\n",
      "Forward/backward pass size (MB): 152.07\n",
      "Params size (MB): 13.05\n",
      "Estimated Total Size (MB): 168.66\n",
      "===================================================================================================================\n",
      "Epoch   0/30\n",
      "  1848.5s - train_loss: 0.05509 - val_loss: 0.01307\n",
      "Epoch   1/30\n",
      "  1805.5s - train_loss: 0.01300 - val_loss: 0.01305\n",
      "Epoch   2/30\n",
      "  1807.8s - train_loss: 0.01298 - val_loss: 0.01304\n",
      "Epoch   3/30\n",
      "  1850.0s - train_loss: 0.01297 - val_loss: 0.01303\n",
      "Epoch   4/30\n",
      "  1814.8s - train_loss: 0.01297 - val_loss: 0.01302\n",
      "Epoch   5/30\n",
      "  1816.6s - train_loss: 0.01296 - val_loss: 0.01300\n",
      "Epoch   6/30\n",
      "  1817.4s - train_loss: 0.01294 - val_loss: 0.01299\n",
      "Epoch   7/30\n",
      "  1819.7s - train_loss: 0.01292 - val_loss: 0.01295\n",
      "Epoch   8/30\n",
      "  1820.1s - train_loss: 0.01288 - val_loss: 0.01290\n",
      "Epoch   9/30\n",
      "  1825.3s - train_loss: 0.01282 - val_loss: 0.01288\n",
      "Epoch  10/30\n",
      "  1835.3s - train_loss: 0.01276 - val_loss: 0.01279\n",
      "Epoch  11/30\n",
      "  1868.7s - train_loss: 0.01272 - val_loss: 0.01277\n",
      "Epoch  12/30\n",
      "  1937.8s - train_loss: 0.01268 - val_loss: 0.01272\n",
      "Epoch  13/30\n",
      "  2059.2s - train_loss: 0.01265 - val_loss: 0.01269\n",
      "Epoch  14/30\n",
      "  2175.5s - train_loss: 0.01262 - val_loss: 0.01265\n",
      "Epoch  15/30\n",
      "  2189.8s - train_loss: 0.01259 - val_loss: 0.01263\n",
      "Epoch  16/30\n",
      "  2193.5s - train_loss: 0.01255 - val_loss: 0.01259\n",
      "Epoch  17/30\n",
      "  2196.2s - train_loss: 0.01252 - val_loss: 0.01257\n"
     ]
    }
   ],
   "source": [
    "## create NN architecture\n",
    "model = NeuralNetwork(config=config[\"arch_atm\"])\n",
    "\n",
    "## grab optimizer and loss \n",
    "optimizer = getattr(torch.optim, config[\"optimizer\"][\"type\"])(\n",
    "    model.parameters(), **config[\"optimizer\"][\"args\"]\n",
    ")\n",
    "criterion = getattr(torch.nn, config[\"criterion\"])()\n",
    "\n",
    "metric_funcs = [getattr(module_metric, met) for met in config[\"metrics\"]]\n",
    "\n",
    "## Build the trainer\n",
    "device = prepare_device(config[\"device\"])\n",
    "trainer = Trainer(\n",
    "    model,\n",
    "    criterion,\n",
    "    metric_funcs,\n",
    "    optimizer,\n",
    "    max_epochs=config[\"trainer\"][\"max_epochs\"],\n",
    "    data=train_dataloader,\n",
    "    validation_data=val_dataloader,\n",
    "    device=device,\n",
    "    config=config,\n",
    ")\n",
    "\n",
    "# Visualize the model\n",
    "torchinfo.summary(\n",
    "    model,\n",
    "    input_size=(config[\"data_loader\"][\"batch_size\"], 2, 96, 144),\n",
    "    verbose=1,\n",
    "    col_names=(\"input_size\", \"output_size\", \"num_params\"),\n",
    ")\n",
    "\n",
    "#Train the Model\n",
    "model.to(device)\n",
    "trainer.fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a2b8773-4894-4e8c-97fb-5c6ee580933e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(trainer.log.history.keys())\n",
    "\n",
    "plt.figure(figsize=(20, 4))\n",
    "for i, m in enumerate((\"loss\", *config[\"metrics\"])):\n",
    "    plt.subplot(1, 4, i + 1)\n",
    "    plt.plot(trainer.log.history[\"epoch\"], trainer.log.history[m], label=m)\n",
    "    plt.plot(\n",
    "        trainer.log.history[\"epoch\"], trainer.log.history[\"val_\" + m], label=\"val_\" + m\n",
    "    )\n",
    "    plt.axvline(\n",
    "        x=trainer.early_stopper.best_epoch, linestyle=\"--\", color=\"k\", linewidth=0.75\n",
    "    )\n",
    "    plt.title(m)\n",
    "    plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a9d02f2-5e38-4fdd-baef-254e5ba09ed9",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "preds = model(x_test)\n",
    "print(preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ab0c967-dacf-49ef-aaf8-c3d350e94ff3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MLWPS",
   "language": "python",
   "name": "mlwps"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
