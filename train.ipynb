{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "959cb75a-32de-4d34-bcdc-4687a1c9f155",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_dir = '/glade/work/kjmayer/research/catalyst/S2S_ocn_lnd_atm/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "186b954f-dd38-495a-a5e7-30b0cced502b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import xarray as xr\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "import torch\n",
    "import torchinfo\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "from model.train_utils import NeuralNetwork\n",
    "import utils.utils\n",
    "from utils.utils import get_config\n",
    "from utils.utils import prepare_device\n",
    "from trainer.trainer import Trainer\n",
    "import model.metrics as module_metric\n",
    "from data_prep.data_loader import GetData,MakeDataset,lead_shift, concat_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9b1c9670-cf98-4379-bf1c-0cc3be7bcb80",
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "56953f61-bc6e-477e-bcce-6a5cf23d21f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = get_config(\"exp_1\")\n",
    "\n",
    "torch.manual_seed(config[\"seed\"])\n",
    "torch.cuda.manual_seed(config[\"seed\"])\n",
    "np.random.seed(config[\"seed\"])\n",
    "random.seed(config[\"seed\"])\n",
    "torch.backends.cudnn.deterministic = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3e0c151f-25a5-4d4c-ad70-d1f3ddd432a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/glade/work/wchapman/miniconda3.1/envs/MLWPS/lib/python3.11/site-packages/xarray/core/variable.py:1546: PerformanceWarning: Reshaping is producing a large chunk. To accept the large\n",
      "chunk and silence this warning, set the option\n",
      "    >>> with dask.config.set(**{'array.slicing.split_large_chunks': False}):\n",
      "    ...     array.reshape(shape)\n",
      "\n",
      "To avoid creating the large chunks, set the option\n",
      "    >>> with dask.config.set(**{'array.slicing.split_large_chunks': True}):\n",
      "    ...     array.reshape(shape)Explictly passing ``limit`` to ``reshape`` will also silence this warning\n",
      "    >>> array.reshape(shape, limit='128 MiB')\n",
      "  result = result._stack_once(dims, new_dim)\n",
      "/glade/work/wchapman/miniconda3.1/envs/MLWPS/lib/python3.11/site-packages/xarray/core/variable.py:1546: PerformanceWarning: Reshaping is producing a large chunk. To accept the large\n",
      "chunk and silence this warning, set the option\n",
      "    >>> with dask.config.set(**{'array.slicing.split_large_chunks': False}):\n",
      "    ...     array.reshape(shape)\n",
      "\n",
      "To avoid creating the large chunks, set the option\n",
      "    >>> with dask.config.set(**{'array.slicing.split_large_chunks': True}):\n",
      "    ...     array.reshape(shape)Explictly passing ``limit`` to ``reshape`` will also silence this warning\n",
      "    >>> array.reshape(shape, limit='128 MiB')\n",
      "  result = result._stack_once(dims, new_dim)\n",
      "/glade/work/wchapman/miniconda3.1/envs/MLWPS/lib/python3.11/site-packages/xarray/core/variable.py:1546: PerformanceWarning: Reshaping is producing a large chunk. To accept the large\n",
      "chunk and silence this warning, set the option\n",
      "    >>> with dask.config.set(**{'array.slicing.split_large_chunks': False}):\n",
      "    ...     array.reshape(shape)\n",
      "\n",
      "To avoid creating the large chunks, set the option\n",
      "    >>> with dask.config.set(**{'array.slicing.split_large_chunks': True}):\n",
      "    ...     array.reshape(shape)Explictly passing ``limit`` to ``reshape`` will also silence this warning\n",
      "    >>> array.reshape(shape, limit='128 MiB')\n",
      "  result = result._stack_once(dims, new_dim)\n",
      "/glade/work/wchapman/miniconda3.1/envs/MLWPS/lib/python3.11/site-packages/xarray/core/variable.py:1546: PerformanceWarning: Reshaping is producing a large chunk. To accept the large\n",
      "chunk and silence this warning, set the option\n",
      "    >>> with dask.config.set(**{'array.slicing.split_large_chunks': False}):\n",
      "    ...     array.reshape(shape)\n",
      "\n",
      "To avoid creating the large chunks, set the option\n",
      "    >>> with dask.config.set(**{'array.slicing.split_large_chunks': True}):\n",
      "    ...     array.reshape(shape)Explictly passing ``limit`` to ``reshape`` will also silence this warning\n",
      "    >>> array.reshape(shape, limit='128 MiB')\n",
      "  result = result._stack_once(dims, new_dim)\n"
     ]
    }
   ],
   "source": [
    "LEAD = 7 # will loop over this for training eventually\n",
    "trainfinames = config[\"data_loader\"][\"anommems_finames\"][0:6]\n",
    "valfinames = config[\"data_loader\"][\"anommems_finames\"][6:8]\n",
    "\n",
    "xtrain, xtrainmean, xtrainstd, xtrainmin, xtrainmax = GetData(dir=config[\"data_loader\"][\"base_dir\"],\n",
    "                                                              var=config[\"data_loader\"][\"atm_var\"],\n",
    "                                                              finames=trainfinames,\n",
    "                                                              train=True,\n",
    "                                                              climo=False)[0]\n",
    "# xtrain = xtrain.stack(l=('lat','lon'))\n",
    "xtrain_shift = lead_shift(xtrain, lead=LEAD, forward=False)\n",
    "ytrain = xtrain.sel(lat=slice(29,61))\n",
    "ytrain = ytrain.stack(l=('lat','lon'))\n",
    "ytrain_shift = lead_shift(ytrain, lead=LEAD, forward=True)\n",
    "\n",
    "xval = GetData(dir=config[\"data_loader\"][\"base_dir\"],\n",
    "               var=config[\"data_loader\"][\"atm_var\"],\n",
    "               finames=valfinames,\n",
    "               train=False,\n",
    "               trainmean=xtrainmean,\n",
    "               trainstd=xtrainstd,\n",
    "               trainmin=xtrainmin,\n",
    "               trainmax=xtrainmax,\n",
    "               climo=False)[0]\n",
    "# \n",
    "xval_shift = lead_shift(xval, lead=LEAD, forward=False)\n",
    "yval = xval.sel(lat=slice(29,61))\n",
    "yval = yval.stack(l=('lat','lon'))\n",
    "yval_shift = lead_shift(yval, lead=LEAD, forward=True)\n",
    "\n",
    "\n",
    "# climo (same for train, val, and test --> basically a DOY encoder)\n",
    "xclimo, climomin, climomax = GetData(dir = config[\"data_loader\"][\"base_dir\"],\n",
    "                                     var = config[\"data_loader\"][\"atm_var\"],\n",
    "                                     finames = config[\"data_loader\"][\"climo_finame\"],\n",
    "                                     train = False, # MUST ALWAYS BE False FOR CLIMO\n",
    "                                     climo = True)[0]\n",
    "# climo appended to same length as training\n",
    "xclimo = xclimo.rename({'dayofyear': 'time'})\n",
    "xclimo_train = xr.concat([xclimo]*int(xtrain.shape[0]/365),dim='mem')\n",
    "xclimo_train = xclimo_train.stack(s=('mem','time')).transpose('s', 'lat', 'lon').reset_index(['s'])\n",
    "xclimo_train_shift = lead_shift(xclimo_train,lead=LEAD,forward=False)\n",
    "\n",
    "# climo appended to same length as validation\n",
    "xclimo_val = xr.concat([xclimo]*int(xval.shape[0]/365),dim='mem')\n",
    "xclimo_val = xclimo_val.stack(s=('mem','time')).transpose('s', 'lat', 'lon').reset_index(['s'])\n",
    "xclimo_val_shift = lead_shift(xclimo_val,lead=LEAD,forward=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fea1995c-49af-4621-8b7f-3b7a10d2659a",
   "metadata": {},
   "outputs": [],
   "source": [
    "Xtrain = concat_input(xtrain_shift,xclimo_train_shift,dim_name='features').values\n",
    "del xtrain_shift,xclimo_train_shift\n",
    "Xval = concat_input(xval_shift,xclimo_val_shift,dim_name='features').values\n",
    "del xval_shift,xclimo_val_shift\n",
    "Ytrain = ytrain_shift.values\n",
    "del ytrain_shift\n",
    "Yval = yval_shift.values\n",
    "del yval_shift"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "56bbd262-3ba4-44f5-b4a1-1f3f60a104f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/glade/work/kjmayer/research/catalyst/S2S_ocn_lnd_atm/data_prep/data_loader.py:83: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  self.X = torch.tensor(torch.from_numpy(X), dtype = torch.float32)#.unsqueeze(1)\n",
      "/glade/work/kjmayer/research/catalyst/S2S_ocn_lnd_atm/data_prep/data_loader.py:84: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  self.y = torch.tensor(torch.from_numpy(X), dtype = torch.float32) # unsqueeze?\n"
     ]
    }
   ],
   "source": [
    "## Prep training and validation for ANN\n",
    "training_data = MakeDataset(Xtrain,Ytrain) #xtrain, ytrain need to be numpy, not xarray\n",
    "val_data = MakeDataset(Xval,Yval)\n",
    "\n",
    "#[batch_size,channels,lat,lon]\n",
    "train_dataloader = DataLoader(training_data,batch_size = config[\"data_loader\"][\"batch_size\"],shuffle=True)\n",
    "val_dataloader  = DataLoader(val_data,batch_size = config[\"data_loader\"][\"batch_size\"],shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8beea90f-9f06-4079-960f-7bacc718d7c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "for input, output in val_dataloader:\n",
    "    print(input)\n",
    "    print(output)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "11a40d16-8128-4396-9eed-cd274813ff00",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'Flatten' object has no attribute 'size'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m## create NN architecture\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mNeuralNetwork\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43march_atm\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m## grab optimizer and loss \u001b[39;00m\n\u001b[1;32m      5\u001b[0m optimizer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(torch\u001b[38;5;241m.\u001b[39moptim, config[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moptimizer\u001b[39m\u001b[38;5;124m\"\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtype\u001b[39m\u001b[38;5;124m\"\u001b[39m])(\n\u001b[1;32m      6\u001b[0m     model\u001b[38;5;241m.\u001b[39mparameters(), \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mconfig[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moptimizer\u001b[39m\u001b[38;5;124m\"\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124margs\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m      7\u001b[0m )\n",
      "File \u001b[0;32m/glade/work/kjmayer/research/catalyst/S2S_ocn_lnd_atm/model/train_utils.py:76\u001b[0m, in \u001b[0;36mNeuralNetwork.__init__\u001b[0;34m(self, config)\u001b[0m\n\u001b[1;32m     74\u001b[0m \u001b[38;5;66;03m# Flat layer\u001b[39;00m\n\u001b[1;32m     75\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mflat \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mFlatten(start_dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m---> 76\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mflat\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msize\u001b[49m())\n\u001b[1;32m     77\u001b[0m \u001b[38;5;66;03m# Input Dense blocks\u001b[39;00m\n\u001b[1;32m     78\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdenseblock \u001b[38;5;241m=\u001b[39m dense_block(\n\u001b[1;32m     79\u001b[0m     config[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhiddens_block\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m     80\u001b[0m     config[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhiddens_block_act\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m     81\u001b[0m     in_features\u001b[38;5;241m=\u001b[39mconfig[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhiddens_block_in\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m     82\u001b[0m )\n",
      "File \u001b[0;32m/glade/work/wchapman/miniconda3.1/envs/MLWPS/lib/python3.11/site-packages/torch/nn/modules/module.py:1695\u001b[0m, in \u001b[0;36mModule.__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   1693\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m modules:\n\u001b[1;32m   1694\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m modules[name]\n\u001b[0;32m-> 1695\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m object has no attribute \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'Flatten' object has no attribute 'size'"
     ]
    }
   ],
   "source": [
    "## create NN architecture\n",
    "model = NeuralNetwork(config=config[\"arch_atm\"])\n",
    "\n",
    "## grab optimizer and loss \n",
    "optimizer = getattr(torch.optim, config[\"optimizer\"][\"type\"])(\n",
    "    model.parameters(), **config[\"optimizer\"][\"args\"]\n",
    ")\n",
    "criterion = getattr(torch.nn, config[\"criterion\"])()\n",
    "\n",
    "metric_funcs = [getattr(module_metric, met) for met in config[\"metrics\"]]\n",
    "\n",
    "## Build the trainer\n",
    "device = prepare_device(config[\"device\"])\n",
    "trainer = Trainer(\n",
    "    model,\n",
    "    criterion,\n",
    "    metric_funcs,\n",
    "    optimizer,\n",
    "    max_epochs=config[\"trainer\"][\"max_epochs\"],\n",
    "    data=train_dataloader,\n",
    "    validation_data=val_dataloader,\n",
    "    device=device,\n",
    "    config=config,\n",
    ")\n",
    "\n",
    "# Visualize the model\n",
    "torchinfo.summary(\n",
    "    model,\n",
    "    input_size=(config[\"data_loader\"][\"batch_size\"], 2448),\n",
    "    verbose=1,\n",
    "    col_names=(\"input_size\", \"output_size\", \"num_params\"),\n",
    ")\n",
    "\n",
    "# Train the Model\n",
    "# model.to(device)\n",
    "# trainer.fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a2b8773-4894-4e8c-97fb-5c6ee580933e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(trainer.log.history.keys())\n",
    "\n",
    "plt.figure(figsize=(20, 4))\n",
    "for i, m in enumerate((\"loss\", *config[\"metrics\"])):\n",
    "    plt.subplot(1, 4, i + 1)\n",
    "    plt.plot(trainer.log.history[\"epoch\"], trainer.log.history[m], label=m)\n",
    "    plt.plot(\n",
    "        trainer.log.history[\"epoch\"], trainer.log.history[\"val_\" + m], label=\"val_\" + m\n",
    "    )\n",
    "    plt.axvline(\n",
    "        x=trainer.early_stopper.best_epoch, linestyle=\"--\", color=\"k\", linewidth=0.75\n",
    "    )\n",
    "    plt.title(m)\n",
    "    plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a9d02f2-5e38-4fdd-baef-254e5ba09ed9",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "preds = model(x_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MLWPS",
   "language": "python",
   "name": "mlwps"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
